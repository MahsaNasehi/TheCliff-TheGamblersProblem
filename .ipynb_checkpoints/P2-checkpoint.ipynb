{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid from sutton, page 163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: <object object at 0x0000025B29D17280>\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range(values, vmin=0, vmax=1):\n",
    "    start_zero = values - np.min(values)\n",
    "    return (start_zero / (np.max(start_zero) + 1e-7)) * (vmax - vmin) + vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    terrain_color = dict(normal=[127/360, 0, 96/100],\n",
    "                         objective=[26/360, 100/100, 100/100],\n",
    "                         cliff=[247/360, 92/100, 70/100],\n",
    "                         player=[344/360, 93/100, 100/100])\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.player = None\n",
    "        self._create_grid()  \n",
    "        self._draw_grid()\n",
    "        \n",
    "    def _create_grid(self, initial_grid=None):\n",
    "        self.grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "        self._add_objectives(self.grid)\n",
    "        \n",
    "    def _add_objectives(self, grid):\n",
    "        grid[-1, 1:11] = self.terrain_color['cliff']\n",
    "        grid[-1, -1] = self.terrain_color['objective']\n",
    "        \n",
    "    def _draw_grid(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 4))\n",
    "        self.ax.grid(which='minor')       \n",
    "        self.q_texts = [self.ax.text(*self._id_to_position(i)[::-1], '0',\n",
    "                                     fontsize=11, verticalalignment='center', \n",
    "                                     horizontalalignment='center') for i in range(12 * 4)]     \n",
    "         \n",
    "        self.im = self.ax.imshow(hsv_to_rgb(self.grid), cmap='terrain',\n",
    "                                 interpolation='nearest', vmin=0, vmax=1)        \n",
    "        self.ax.set_xticks(np.arange(12))\n",
    "        self.ax.set_xticks(np.arange(12) - 0.5, minor=True)\n",
    "        self.ax.set_yticks(np.arange(4))\n",
    "        self.ax.set_yticks(np.arange(4) - 0.5, minor=True)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (3, 0)        \n",
    "        return self._position_to_id(self.player)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        if action == 0 and self.player[0] > 0:\n",
    "            self.player = (self.player[0] - 1, self.player[1])\n",
    "        if action == 1 and self.player[0] < 3:\n",
    "            self.player = (self.player[0] + 1, self.player[1])\n",
    "        if action == 2 and self.player[1] < 11:\n",
    "            self.player = (self.player[0], self.player[1] + 1)\n",
    "        if action == 3 and self.player[1] > 0:\n",
    "            self.player = (self.player[0], self.player[1] - 1)\n",
    "            \n",
    "        # Rules\n",
    "        if all(self.grid[self.player] == self.terrain_color['cliff']):\n",
    "            reward = -100\n",
    "            done = True\n",
    "        elif all(self.grid[self.player] == self.terrain_color['objective']):\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            \n",
    "        return self._position_to_id(self.player), reward, done\n",
    "    \n",
    "    def _position_to_id(self, pos):\n",
    "        ''' Maps a position in x,y coordinates to a unique ID '''\n",
    "        return pos[0] * 12 + pos[1]\n",
    "    \n",
    "    def _id_to_position(self, idx):\n",
    "        return (idx // 12), (idx % 12)\n",
    "        \n",
    "    def render(self, q_values=None, action=None, max_q=False, colorize_q=False):\n",
    "        assert self.player is not None, 'You first need to call .reset()'  \n",
    "        \n",
    "        if colorize_q:\n",
    "            assert q_values is not None, 'q_values must not be None for using colorize_q'            \n",
    "            grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "            values = change_range(np.max(q_values, -1)).reshape(4, 12)\n",
    "            grid[:, :, 1] = values\n",
    "            self._add_objectives(grid)\n",
    "        else:            \n",
    "            grid = self.grid.copy()\n",
    "            \n",
    "        grid[self.player] = self.terrain_color['player']       \n",
    "        self.im.set_data(hsv_to_rgb(grid))\n",
    "               \n",
    "        if q_values is not None:\n",
    "            xs = np.repeat(np.arange(12), 4)\n",
    "            ys = np.tile(np.arange(4), 12)  \n",
    "            \n",
    "            for i, text in enumerate(self.q_texts):\n",
    "                if max_q:\n",
    "                    q = max(q_values[i])    \n",
    "                    txt = '{:.2f}'.format(q)\n",
    "                    text.set_text(txt)\n",
    "                else:                \n",
    "                    actions = ['U', 'D', 'R', 'L']\n",
    "                    txt = '\\n'.join(['{}: {:.2f}'.format(k, q) for k, q in zip(actions, q_values[i])])\n",
    "                    text.set_text(txt)\n",
    "                \n",
    "        if action is not None:\n",
    "            self.ax.set_title(action, color='r', weight='bold', fontsize=32)\n",
    "\n",
    "        plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "actions = ['UP', 'DOWN', 'RIGHT', 'LEFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid\n",
    "\n",
    "<img src=\"imges/fig1.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a table of values that maps each state-action pair to a value, we'll create such table and initialize all values to zero (or to a random value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of states in simply the number of \"squares\" in our grid world, in this case 4 * 12\n",
    "num_states = 4 * 12\n",
    "# We have 4 possible actions, up, down, right and left\n",
    "num_actions = 4\n",
    "\n",
    "q_values = np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(q_values, columns=[' up ', 'down', 'right', 'left'])\n",
    "df.index.name = 'States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up</th>\n",
       "      <th>down</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>States</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         up   down  right  left\n",
       "States                         \n",
       "0        0.0   0.0    0.0   0.0\n",
       "1        0.0   0.0    0.0   0.0\n",
       "2        0.0   0.0    0.0   0.0\n",
       "3        0.0   0.0    0.0   0.0\n",
       "4        0.0   0.0    0.0   0.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe talk on why we need exploration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploitation: Focuses on using the knowledge already gathered to maximize immediate rewards.\n",
    "\n",
    "Exploration: Involves trying new actions to discover potentially better strategies or states.\n",
    "\n",
    "Without exploration, the agent risks getting stuck in suboptimal policies because it never tries untested actions that might yield higher rewards in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(q_values, state, epsilon=0.1):\n",
    "    ''' \n",
    "    Choose an action based on a epsilon greedy policy.    \n",
    "    A random action is selected with epsilon probability, else select the best action.    \n",
    "    Returns:\n",
    "        action --type(int)\n",
    "    '''\n",
    "    # choose a randon integer from [0, 1)\n",
    "    explore_or_exploit = np.random.random()\n",
    "    if explore_or_exploit < epsilon: # do random action\n",
    "        # randomly select an integer from the range [0, 1, 2, 3]\n",
    "        action = np.random.choice(4)\n",
    "    else:\n",
    "        action = np.argmax(q_values[state, :])\n",
    "    return action        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=500, render=True, exploration_rate=0.1,\n",
    "               learning_rate=0.5, gamma=0.9):    \n",
    "    q_values = np.zeros((num_states, num_actions))\n",
    "    ep_rewards = [] # episode_rewards\n",
    "    \n",
    "    ## TODO\n",
    "    for episode in range(0, num_episodes):\n",
    "        state = env.reset()\n",
    "        reach_terminal_state = False\n",
    "        sum_of_rewards = 0\n",
    "        while not reach_terminal_state:\n",
    "            action = egreedy_policy(q_values, state, exploration_rate)\n",
    "            # do the action\n",
    "            next_state , reward, reach_terminal_state = env.step(action)\n",
    "            sum_of_rewards += reward\n",
    "\n",
    "            sample = reward + gamma * np.max(q_values[next_state, :])\n",
    "            q_values[state][action] = (1 - learning_rate) * q_values[state][action] + learning_rate * sample\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # # if want to render\n",
    "            # if render:\n",
    "            #     env.render(q_values, action=actions[action], colorize_q=True)\n",
    "        ep_rewards.append(sum_of_rewards)\n",
    "\n",
    "    return ep_rewards, q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_rewards, q_values = q_learning(env, gamma=0.9, learning_rate=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(q_values, colorize_q=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output\n",
    "for q-learning\n",
    "\n",
    "<img src=\"imges/fig2.png\" width=\"400\" height=\"200\">\n",
    "<img src=\"imges/fig2_render.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39.14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(q_learning_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -41.40820000000001\n"
     ]
    }
   ],
   "source": [
    "q_learning_rewards, _ = zip(*[q_learning(env, render=False, exploration_rate=0.1,\n",
    "                                         learning_rate=1) for _ in range(10)])\n",
    "avg_rewards = np.mean(q_learning_rewards, axis=0)\n",
    "mean_reward = [np.mean(avg_rewards)] * len(avg_rewards)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.plot(avg_rewards)\n",
    "ax.plot(mean_reward, 'g--')\n",
    "\n",
    "print('Mean Reward: {}'.format(mean_reward[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA learning\n",
    "For a learning agent in any Reinforcement Learning algorithm it’s policy can be of two types:\n",
    "1.On Policy: In this, the learning agent learns the value function according to the current action derived from the policy currently being used.\n",
    "2.Off Policy: In this, the learning agent learns the value function according to the action derived from another policy.\n",
    "\n",
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
    "\n",
    "## Q learning:\n",
    "Q(s, a) <- (1 - learning_rate) * Q(s, a) + learning_rate * (reward + gamma + max_a'(Q(s', a')))\n",
    "## SARSA learning:\n",
    "Q(s, a) <- (1 - learning_rate) * Q(s, a) + learning_rate * (reward + gamma + Q(s', a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes=500, render=True, exploration_rate=0.1,\n",
    "          learning_rate=0.5, gamma=0.9):\n",
    "    q_values_sarsa = np.zeros((num_states, num_actions))\n",
    "    ep_rewards = []\n",
    "    \n",
    "    ## TODO\n",
    "    for episode in range(0, num_episodes):\n",
    "        state = env.reset()\n",
    "        reach_terminal_state = False\n",
    "        sum_of_rewards = 0\n",
    "        action = egreedy_policy(q_values_sarsa, state, exploration_rate)\n",
    "        while not reach_terminal_state:\n",
    "            # do the action\n",
    "            next_state , reward, reach_terminal_state = env.step(action)\n",
    "            sum_of_rewards += reward\n",
    "\n",
    "            # do the next action\n",
    "            next_action = egreedy_policy(q_values_sarsa, state, exploration_rate)\n",
    "\n",
    "            sample = reward + gamma * q_values_sarsa[next_state, next_action]\n",
    "            q_values_sarsa[state][action] = (1 - learning_rate) * q_values_sarsa[state][action] + learning_rate * sample\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            # # if wanna render\n",
    "            # if render:\n",
    "            #     env.render(q_values, action=actions[action], colorize_q=True)\n",
    "        ep_rewards.append(sum_of_rewards)  \n",
    "    return ep_rewards, q_values_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_rewards, q_values_sarsa = sarsa(env, render=True, learning_rate=0.5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-66.4096"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sarsa_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sarsa_rewards, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m[\u001b[43msarsa\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m)]) \u001b[38;5;66;03m# for in range(100)\u001b[39;00m\n\u001b[0;32m      3\u001b[0m avg_rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(sarsa_rewards, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m mean_reward \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39mmean(avg_rewards)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(avg_rewards)\n",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m, in \u001b[0;36msarsa\u001b[1;34m(env, num_episodes, render, exploration_rate, learning_rate, gamma)\u001b[0m\n\u001b[0;32m     15\u001b[0m sum_of_rewards \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# do the next action\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m next_action \u001b[38;5;241m=\u001b[39m \u001b[43megreedy_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values_sarsa\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexploration_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m sample \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m q_values_sarsa[next_state, next_action]\n\u001b[0;32m     21\u001b[0m q_values_sarsa[state][action] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m learning_rate) \u001b[38;5;241m*\u001b[39m q_values_sarsa[state][action] \u001b[38;5;241m+\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m sample\n",
      "Cell \u001b[1;32mIn[10], line 14\u001b[0m, in \u001b[0;36megreedy_policy\u001b[1;34m(q_values, state, epsilon)\u001b[0m\n\u001b[0;32m     12\u001b[0m     action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(\u001b[38;5;241m4\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sarsa_rewards, _ = zip(*[sarsa(env, render=False, exploration_rate=0.1) for _ in range(10)]) # for in range(100)\n",
    "\n",
    "avg_rewards = np.mean(sarsa_rewards, axis=0)\n",
    "mean_reward = [np.mean(avg_rewards)] * len(avg_rewards)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.plot(avg_rewards)\n",
    "ax.plot(mean_reward, 'g--')\n",
    "\n",
    "print('Mean Reward: {}'.format(mean_reward[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(q_values):\n",
    "    env = GridWorld()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:    \n",
    "        # Select action\n",
    "        action = egreedy_policy(q_values, state, 0.0)\n",
    "        # Do the action\n",
    "        next_state, reward, done = env.step(action)  \n",
    "\n",
    "        # Update state and action        \n",
    "        state = next_state  \n",
    "        \n",
    "        env.render(q_values=q_values, action=actions[action], colorize_q=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(q_values_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output\n",
    "for sarsa-learning\n",
    "\n",
    "<img src=\"imges/fig3.png\" width=\"400\" height=\"200\">\n",
    "<img src=\"imges/fig3_render.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
