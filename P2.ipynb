{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid from sutton, page 163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: TkAgg\n"
     ]
    }
   ],
   "source": [
    "%matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import hsv_to_rgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_range(values, vmin=0, vmax=1):\n",
    "    start_zero = values - np.min(values)\n",
    "    return (start_zero / (np.max(start_zero) + 1e-7)) * (vmax - vmin) + vmin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridWorld:\n",
    "    terrain_color = dict(normal=[127/360, 0, 96/100],\n",
    "                         objective=[26/360, 100/100, 100/100],\n",
    "                         cliff=[247/360, 92/100, 70/100],\n",
    "                         player=[344/360, 93/100, 100/100])\n",
    "        \n",
    "    def __init__(self):\n",
    "        self.player = None\n",
    "        self._create_grid()  \n",
    "        self._draw_grid()\n",
    "        \n",
    "    def _create_grid(self, initial_grid=None):\n",
    "        self.grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "        self._add_objectives(self.grid)\n",
    "        \n",
    "    def _add_objectives(self, grid):\n",
    "        grid[-1, 1:11] = self.terrain_color['cliff']\n",
    "        grid[-1, -1] = self.terrain_color['objective']\n",
    "        \n",
    "    def _draw_grid(self):\n",
    "        self.fig, self.ax = plt.subplots(figsize=(12, 4))\n",
    "        self.ax.grid(which='minor')       \n",
    "        self.q_texts = [self.ax.text(*self._id_to_position(i)[::-1], '0',\n",
    "                                     fontsize=11, verticalalignment='center', \n",
    "                                     horizontalalignment='center') for i in range(12 * 4)]     \n",
    "         \n",
    "        self.im = self.ax.imshow(hsv_to_rgb(self.grid), cmap='terrain',\n",
    "                                 interpolation='nearest', vmin=0, vmax=1)        \n",
    "        self.ax.set_xticks(np.arange(12))\n",
    "        self.ax.set_xticks(np.arange(12) - 0.5, minor=True)\n",
    "        self.ax.set_yticks(np.arange(4))\n",
    "        self.ax.set_yticks(np.arange(4) - 0.5, minor=True)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.player = (3, 0)        \n",
    "        return self._position_to_id(self.player)\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Possible actions\n",
    "        if action == 0 and self.player[0] > 0:\n",
    "            self.player = (self.player[0] - 1, self.player[1])\n",
    "        if action == 1 and self.player[0] < 3:\n",
    "            self.player = (self.player[0] + 1, self.player[1])\n",
    "        if action == 2 and self.player[1] < 11:\n",
    "            self.player = (self.player[0], self.player[1] + 1)\n",
    "        if action == 3 and self.player[1] > 0:\n",
    "            self.player = (self.player[0], self.player[1] - 1)\n",
    "            \n",
    "        # Rules\n",
    "        if all(self.grid[self.player] == self.terrain_color['cliff']):\n",
    "            reward = -100\n",
    "            done = True\n",
    "        elif all(self.grid[self.player] == self.terrain_color['objective']):\n",
    "            reward = 0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -1\n",
    "            done = False\n",
    "            \n",
    "        return self._position_to_id(self.player), reward, done\n",
    "    \n",
    "    def _position_to_id(self, pos):\n",
    "        ''' Maps a position in x,y coordinates to a unique ID '''\n",
    "        return pos[0] * 12 + pos[1]\n",
    "    \n",
    "    def _id_to_position(self, idx):\n",
    "        return (idx // 12), (idx % 12)\n",
    "        \n",
    "    def render(self, q_values=None, action=None, max_q=False, colorize_q=False):\n",
    "        assert self.player is not None, 'You first need to call .reset()'  \n",
    "        \n",
    "        if colorize_q:\n",
    "            assert q_values is not None, 'q_values must not be None for using colorize_q'            \n",
    "            grid = self.terrain_color['normal'] * np.ones((4, 12, 3))\n",
    "            values = change_range(np.max(q_values, -1)).reshape(4, 12)\n",
    "            grid[:, :, 1] = values\n",
    "            self._add_objectives(grid)\n",
    "        else:            \n",
    "            grid = self.grid.copy()\n",
    "            \n",
    "        grid[self.player] = self.terrain_color['player']       \n",
    "        self.im.set_data(hsv_to_rgb(grid))\n",
    "               \n",
    "        if q_values is not None:\n",
    "            xs = np.repeat(np.arange(12), 4)\n",
    "            ys = np.tile(np.arange(4), 12)  \n",
    "            \n",
    "            for i, text in enumerate(self.q_texts):\n",
    "                if max_q:\n",
    "                    q = max(q_values[i])    \n",
    "                    txt = '{:.2f}'.format(q)\n",
    "                    text.set_text(txt)\n",
    "                else:                \n",
    "                    actions = ['U', 'D', 'R', 'L']\n",
    "                    txt = '\\n'.join(['{}: {:.2f}'.format(k, q) for k, q in zip(actions, q_values[i])])\n",
    "                    text.set_text(txt)\n",
    "                \n",
    "        if action is not None:\n",
    "            self.ax.set_title(action, color='r', weight='bold', fontsize=32)\n",
    "\n",
    "        plt.pause(0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "UP = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "LEFT = 3\n",
    "actions = ['UP', 'DOWN', 'RIGHT', 'LEFT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = GridWorld()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid\n",
    "\n",
    "<img src=\"imges/fig1.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a table of values that maps each state-action pair to a value, we'll create such table and initialize all values to zero (or to a random value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of states in simply the number of \"squares\" in our grid world, in this case 4 * 12\n",
    "num_states = 4 * 12\n",
    "# We have 4 possible actions, up, down, right and left\n",
    "num_actions = 4\n",
    "\n",
    "q_values = np.zeros((num_states, num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(q_values, columns=[' up ', 'down', 'right', 'left'])\n",
    "df.index.name = 'States'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>up</th>\n",
       "      <th>down</th>\n",
       "      <th>right</th>\n",
       "      <th>left</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>States</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         up   down  right  left\n",
       "States                         \n",
       "0        0.0   0.0    0.0   0.0\n",
       "1        0.0   0.0    0.0   0.0\n",
       "2        0.0   0.0    0.0   0.0\n",
       "3        0.0   0.0    0.0   0.0\n",
       "4        0.0   0.0    0.0   0.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maybe talk on why we need exploration?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploitation: Focuses on using the knowledge already gathered to maximize immediate rewards.\n",
    "\n",
    "Exploration: Involves trying new actions to discover potentially better strategies or states.\n",
    "\n",
    "Without exploration, the agent risks getting stuck in suboptimal policies because it never tries untested actions that might yield higher rewards in the long run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another exploration function\n",
    "def exploration_function(episode, exploration_rate, max_epsilon=0.3, min_epsilon=0.05):\n",
    "    return max(min_epsilon, max_epsilon - exploration_rate * episode)\n",
    "\n",
    "# # another exploration function \n",
    "# # we need visit_count[state, a], that is not implemented\n",
    "# def optimistic_exploration_function(u, n, k):\n",
    "#     \"\"\"\n",
    "#     Exploration function that returns an optimistic utility.\n",
    "    \n",
    "#     Parameters:\n",
    "#         u (float): Value estimate.\n",
    "#         n (int): Visit count.\n",
    "#         k (float): Exploration bonus weight.\n",
    "\n",
    "#     Returns:\n",
    "#         float: Optimistic utility.\n",
    "#     \"\"\"\n",
    "#     return u + k / (n + 1e-5)  # Small epsilon to avoid division by zero\n",
    "\n",
    "# def egreedy_policy(q_values, visit_count, state, epsilon=0.1, k=1.0):\n",
    "#     ''' \n",
    "#     Epsilon-greedy exploration policy with an optimistic bonus for rarely visited actions.    \n",
    "#     Returns:\n",
    "#         action -- type(int)\n",
    "#     '''\n",
    "#     explore_or_exploit = np.random.random()\n",
    "#     if explore_or_exploit < epsilon:  # do random action\n",
    "#         action = np.random.choice(q_values.shape[1])\n",
    "#     else:\n",
    "#         # Add exploration bonus to Q-values\n",
    "#         optimistic_values = [\n",
    "#             optimistic_exploration_function(q_values[state, a], visit_count[state, a], k)\n",
    "#             for a in range(q_values.shape[1])\n",
    "#         ]\n",
    "#         action = np.argmax(optimistic_values)\n",
    "#     return action\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note that\n",
    "exploration function gives us better results, because it reduces randomness so it converges better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def egreedy_policy(q_values, state, epsilon=0.1):\n",
    "    ''' \n",
    "    Choose an action based on a epsilon greedy policy.    \n",
    "    A random action is selected with epsilon probability, else select the best action.    \n",
    "    Returns:\n",
    "        action --type(int)\n",
    "    '''\n",
    "    # choose a randon integer from [0, 1)\n",
    "    explore_or_exploit = np.random.random()\n",
    "    if explore_or_exploit < epsilon: # do random action\n",
    "        # randomly select an integer from the range [0, 1, 2, 3]\n",
    "        action = np.random.choice(4)\n",
    "    else:\n",
    "        action = np.argmax(q_values[state, :])\n",
    "    return action        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes=500, render=True, exploration_rate=0.1,\n",
    "               learning_rate=0.5, gamma=0.9):    \n",
    "    q_values = np.zeros((num_states, num_actions))\n",
    "    ep_rewards = [] # episode_rewards\n",
    "    \n",
    "    ## TODO\n",
    "    for episode in range(0, num_episodes):\n",
    "        state = env.reset()\n",
    "        reach_terminal_state = False\n",
    "        sum_of_rewards = 0\n",
    "        while not reach_terminal_state:\n",
    "            # using exploration function\n",
    "            # action = egreedy_policy(q_values, state, exploration_function(episode, exploration_rate))\n",
    "            action = egreedy_policy(q_values, state, exploration_rate)\n",
    "            # do the action\n",
    "            next_state , reward, reach_terminal_state = env.step(action)\n",
    "            sum_of_rewards += reward\n",
    "\n",
    "            sample = reward + gamma * np.max(q_values[next_state, :])\n",
    "            q_values[state][action] = (1 - learning_rate) * q_values[state][action] + learning_rate * sample\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # # if want to render\n",
    "            # if render:\n",
    "            #     env.render(q_values, action=actions[action], colorize_q=True)\n",
    "        ep_rewards.append(sum_of_rewards)\n",
    "\n",
    "    return ep_rewards, q_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_learning_rewards, q_values = q_learning(env, gamma=0.9, learning_rate=1, render=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render(q_values, colorize_q=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output\n",
    "for q-learning\n",
    "\n",
    "<img src=\"imges/fig2.png\" width=\"400\" height=\"200\">\n",
    "<img src=\"imges/fig2_render.png\" width=\"400\" height=\"200\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39.14"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(q_learning_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -41.40820000000001\n"
     ]
    }
   ],
   "source": [
    "q_learning_rewards, _ = zip(*[q_learning(env, render=False, exploration_rate=0.1,\n",
    "                                         learning_rate=1) for _ in range(10)])\n",
    "avg_rewards = np.mean(q_learning_rewards, axis=0)\n",
    "mean_reward = [np.mean(avg_rewards)] * len(avg_rewards)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.plot(avg_rewards)\n",
    "ax.plot(mean_reward, 'g--')\n",
    "\n",
    "print('Mean Reward: {}'.format(mean_reward[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SARSA learning\n",
    "For a learning agent in any Reinforcement Learning algorithm it’s policy can be of two types:\n",
    "\n",
    "1.On Policy: In this, the learning agent learns the value function according to the current action derived from the policy currently being used.\n",
    "\n",
    "2.Off Policy: In this, the learning agent learns the value function according to the action derived from another policy.\n",
    "\n",
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value.\n",
    "\n",
    "## Q learning:\n",
    "Q(s, a) <- (1 - learning_rate) * Q(s, a) + learning_rate * (reward + gamma + max_a'(Q(s', a')))\n",
    "## SARSA learning:\n",
    "Q(s, a) <- (1 - learning_rate) * Q(s, a) + learning_rate * (reward + gamma + Q(s', a'))\n",
    "\n",
    "## Conclusion\n",
    "in the Sarsa learning it chooses the upper path, because it knows that using the optimal path in q-learning sometimes causes to fall into the cliff, and sarsa considers this so it chooses the safer path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarsa(env, num_episodes=500, render=True, exploration_rate=0.1,\n",
    "          learning_rate=0.5, gamma=0.9):\n",
    "    q_values_sarsa = np.zeros((num_states, num_actions))\n",
    "    ep_rewards = []\n",
    "    \n",
    "    ## TODO\n",
    "    for episode in range(0, num_episodes):\n",
    "        state = env.reset()\n",
    "        reach_terminal_state = False\n",
    "        sum_of_rewards = 0\n",
    "        # using exploration function\n",
    "        # action = egreedy_policy(q_values_sarsa, state, exploration_function(episode, exploration_rate))\n",
    "        action = egreedy_policy(q_values_sarsa, state, exploration_rate)\n",
    "        while not reach_terminal_state:\n",
    "            # do the action\n",
    "            next_state , reward, reach_terminal_state = env.step(action)\n",
    "            sum_of_rewards += reward\n",
    "\n",
    "            # do the next action\n",
    "            # using exploration function\n",
    "            # next_action = egreedy_policy(q_values_sarsa, state, exploration_function(episode, exploration_rate))\n",
    "            next_action = egreedy_policy(q_values_sarsa, state, exploration_rate)\n",
    "\n",
    "            sample = reward + gamma * q_values_sarsa[next_state, next_action]\n",
    "            q_values_sarsa[state][action] = (1 - learning_rate) * q_values_sarsa[state][action] + learning_rate * sample\n",
    "\n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "            # # if wanna render\n",
    "            # if render:\n",
    "            #     env.render(q_values, action=actions[action], colorize_q=True)\n",
    "        ep_rewards.append(sum_of_rewards)  \n",
    "    return ep_rewards, q_values_sarsa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_rewards, q_values_sarsa = sarsa(env, render=True, learning_rate=0.5, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-56.464"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(sarsa_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward: -2155.0536\n"
     ]
    }
   ],
   "source": [
    "sarsa_rewards, _ = zip(*[sarsa(env, render=False, exploration_rate=0.1) for _ in range(10)]) # for in range(100)\n",
    "\n",
    "avg_rewards = np.mean(sarsa_rewards, axis=0)\n",
    "mean_reward = [np.mean(avg_rewards)] * len(avg_rewards)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('Rewards')\n",
    "ax.plot(avg_rewards)\n",
    "ax.plot(mean_reward, 'g--')\n",
    "\n",
    "print('Mean Reward: {}'.format(mean_reward[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(q_values):\n",
    "    env = GridWorld()\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:    \n",
    "        # Select action\n",
    "        action = egreedy_policy(q_values, state, 0.0)\n",
    "        # Do the action\n",
    "        next_state, reward, done = env.step(action)  \n",
    "\n",
    "        # Update state and action        \n",
    "        state = next_state  \n",
    "        \n",
    "        env.render(q_values=q_values, action=actions[action], colorize_q=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play(q_values_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# output\n",
    "for sarsa-learning\n",
    "\n",
    "<img src=\"imges/fig3.png\" width=\"400\" height=\"200\">\n",
    "<img src=\"imges/fig3_render.png\" width=\"400\" height=\"200\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
